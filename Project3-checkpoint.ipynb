{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "### Look at sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"leeds.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* explore common tags provided by users\n",
    "* explore essoteric \"ways\" if any exist\n",
    "* explore incomplete street names and fix\n",
    "* source known postal codes for leeds city area\n",
    "* common amenities\n",
    "* expletives and swearwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Audit Street Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Buck Stone Close': ['Buck Streetne Close'],\n",
       " 'Buck Stone Crescent': ['Buck Streetne Crescent'],\n",
       " 'Buck Stone Gardens': ['Buck Streetne Gardens'],\n",
       " 'Buck Stone Green': ['Buck Streetne Green'],\n",
       " 'Buck Stone Mount': ['Buck Streetne Mount'],\n",
       " 'Buck Stone Oval': ['Buck Streetne Oval'],\n",
       " 'Buck Stone Rise': ['Buck Streetne Rise'],\n",
       " 'Buck Stone View': ['Buck Streetne View'],\n",
       " 'Buck Stone Way': ['Buck Streetne Way'],\n",
       " 'Sheepscar Street South': ['Sheepscar Streeteet South'],\n",
       " \"St Wilfrid's Circus\": [\"StreetWilfrid's Circus\"],\n",
       " \"St. Michael's Crescent\": [\"Street Michael's Crescent\"]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osmfile = 'leeds.osm'\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Ave\": \"Avenue\",\n",
    "           \"Ave.\": \"Avenue\",\n",
    "           \"Avenueue\": \"Avenue\"\n",
    "            }\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "addr = re.compile(r'addr:')\n",
    "doubled_colon = re.compile(r':')\n",
    "naptan = re.compile(re.compile(r'naptan'))\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    for incorrect_value, correct in mapping.items():\n",
    "        if re.search(incorrect_value, name):\n",
    "            name = re.sub(incorrect_value, correct, name, count=1)\n",
    "            return name\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def test():\n",
    "    st_types = audit(osmfile)\n",
    "    #print pprint.pprint(st_types.keys())\n",
    "    better_names = {}\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "            for name in ways:\n",
    "                better_name = update_name(name, mapping)\n",
    "                if better_name:\n",
    "                    better_names[name]=[better_name]\n",
    "    print len(better_names)\n",
    "    return better_names\n",
    "    \n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postcode list 0 LS1 1AZ\n",
      "postcode list = 21382 \n",
      "\n",
      "lower_case_postcodes= 0 number of postcodes= 1567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'LS1',\n",
       "  'LS1 4AA',\n",
       "  'LS10 1DU',\n",
       "  'LS10 1JU',\n",
       "  'LS10 1LA',\n",
       "  'LS11',\n",
       "  'LS11 5EF',\n",
       "  'LS14 6',\n",
       "  'LS15 8',\n",
       "  'LS17',\n",
       "  'LS18 4ER',\n",
       "  'LS2 7PJ',\n",
       "  'LS26',\n",
       "  'LS27',\n",
       "  'LS3 1YL',\n",
       "  'LS7',\n",
       "  'LS7 4DP',\n",
       "  'LS8',\n",
       "  'LS8 4BD',\n",
       "  'LS9 0HA'},\n",
       " 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "OSMFILE = \"leeds.osm\"\n",
    "postcode_pattern = re.compile(r'LS')\n",
    "postcode_pattern_lower = re.compile(r'ls')\n",
    "\n",
    "def get_standard_postcodes(postcode_csv):\n",
    "    postcode_list=[]\n",
    "    with open(postcode_csv, 'r') as f:\n",
    "        read = csv.reader(f)\n",
    "        f.next()\n",
    "        for postcode in read:\n",
    "            postcode_list.append(postcode[0])\n",
    "        print \"postcode list 0\",postcode_list[0]\n",
    "    return postcode_list\n",
    "\n",
    "def is_postcode_element(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def is_bradford(elem):\n",
    "    return re.match(r'BD',elem)\n",
    "\n",
    "def audit_postcodes(osm_file, postcode_csv):\n",
    "    number_of_postcodes = 0\n",
    "    lower_case_postcodes = 0\n",
    "    postcode_list = get_standard_postcodes(postcode_csv)\n",
    "    print 'postcode list =',len(postcode_list), '\\n'\n",
    "    osm_file = open(OSMFILE, \"r\")\n",
    "    non_standard_postcodes = set()\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postcode_element(tag):\n",
    "                    number_of_postcodes +=1\n",
    "\n",
    "                    if tag.attrib['v'] not in postcode_list:\n",
    "                        if not is_bradford(tag.attrib['v']):\n",
    "                            non_standard_postcodes.add(tag.attrib['v'])\n",
    "                    if re.match(postcode_pattern_lower, tag.attrib['v']):\n",
    "                        lower_case_postcodes +=1\n",
    "    print 'lower_case_postcodes=',lower_case_postcodes, \"number of postcodes=\", number_of_postcodes\n",
    "    return non_standard_postcodes, lower_case_postcodes\n",
    "    \n",
    "audit_postcodes(OSMFILE, 'ls_postcodes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created': {'changeset': '45753035',\n",
      "             'timestamp': '2017-02-02T17:00:39Z',\n",
      "             'uid': '4543415',\n",
      "             'user': 'The_JF',\n",
      "             'version': '1'},\n",
      " 'golf': 'water_hazard',\n",
      " 'natural': 'water',\n",
      " 'node_refs': ['4656975166',\n",
      "               '4656975167',\n",
      "               '4656975168',\n",
      "               '4656975169',\n",
      "               '4656975166'],\n",
      " 'type': 'way'}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "addr = re.compile(r'addr:')\n",
    "doubled_colon = re.compile(r':')\n",
    "naptan = re.compile(re.compile(r'naptan'))\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "POS = ['lon','lat']\n",
    "\n",
    "def yes_is_true(x):\n",
    "    if x ==\"yes\" or x=='Yes' or x=='YES':\n",
    "        return True\n",
    "    elif x==\"no\" or x==\"No\" or x==\"NO\"\n",
    "        return False\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def shape_element(element):\n",
    "\n",
    "    node = defaultdict(dict)\n",
    "\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "\n",
    "\n",
    "        node['type']=element.tag\n",
    "        node['pos']=[0,0]\n",
    "        node['node_refs']=[]\n",
    "        # YOUR CODE HERE            \n",
    "        for key, value in element.attrib.items():\n",
    "            if key in CREATED:\n",
    "                node['created'][key] = value\n",
    "            elif key in POS:\n",
    "                #print '\\n', key, 'true'\n",
    "                if key == 'lat':\n",
    "                    node['pos'][0]=float(value)\n",
    "                elif key == 'lon':\n",
    "                    node['pos'][1]=float(value)\n",
    "        for child in element.iter():\n",
    "            #print child.tag, child.attrib\n",
    "# put all child entries into respective dictionaries\n",
    "            if child.tag=='tag':\n",
    "                \n",
    "                if re.match(addr,child.attrib['k']) and len(re.findall(doubled_colon,child.attrib['k']))<2:\n",
    "                    node['address']\n",
    "                    node['address'][re.sub(addr,'',child.attrib['k'])] = yes_is_true(child.attrib['v'])\n",
    "                else:\n",
    "                    if re.match(naptan, child.attrib['k']):\n",
    "                        node['naptan:']\n",
    "                        node['naptan:'][re.sub(naptan, '',child.attrib['k'])]=yes_is_true(child.attrib['v'])\n",
    "\n",
    "\n",
    "                    else:    \n",
    "                        node[child.attrib['k']] = yes_is_true(child.attrib['v'])\n",
    "                \n",
    "            if child.tag=='nd':\n",
    "                node['node_refs'].append(child.attrib['ref'])\n",
    "                \n",
    "\n",
    "        if node['pos']==[0,0]:\n",
    "            del node['pos']\n",
    "\n",
    "        if node['k']=={}:\n",
    "            del node['k']\n",
    "        if node['v']=={}:\n",
    "            del node['v']\n",
    "        node = dict(node)\n",
    "\n",
    "        if node['node_refs']==[]:\n",
    "            del node['node_refs']        \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        count =0\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            #print element.tag, element.attrib\n",
    "            \n",
    "            count +=1\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def test():\n",
    "    # NOTE: if you are running this code on your computer, with a larger dataset, \n",
    "    # call the process_map procedure with pretty=False. The pretty=True option adds \n",
    "    # additional spaces to the output, making it significantly larger.\n",
    "    data = process_map('leeds.osm', False)\n",
    "    pprint.pprint(data[-1])\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mongo Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2. Data Overview\n",
    "\n",
    "                                                \n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them.\n",
    "\n",
    "\n",
    "\n",
    "File sizes\n",
    "                                                \n",
    "leeds.osm ......... 117 MB\n",
    "leeds.osm.json .... 117.2 MB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "                                                \n",
    "db.char.find().count()                                                \n",
    "1555851\n",
    "                                                \n",
    "# Number of nodes\n",
    "                                                \n",
    "> db.char.find({\"type\":\"node\"}).count()\n",
    "1471349\n",
    "                                                \n",
    "# Number of ways\n",
    "                                                \n",
    "> db.char.find({\"type\":\"way\"}).count()\n",
    "84502\n",
    "                                                \n",
    "# Number of unique users\n",
    "                                                \n",
    "> db.char.distinct({\"created.user\"}).length\n",
    "336\n",
    "                                                \n",
    "# Top 1 contributing user\n",
    "                                                \n",
    "> db.char.aggregate([{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":­1}}, {\"$limit\":1}])\n",
    "[ { \"_id\" : \"jumbanho\", \"count\" : 823324 } ]                \n",
    "                                                \n",
    "# Number of users appearing only once (having 1 post)\n",
    "                                                \n",
    "> db.char.aggregate([{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, {\"$group\":{\"_id\":\"$count\", \"num_users\":{\"$sum\":1}}}, {\"$sort\":{\"_id\":1}}, {\"$limit\":1}])\n",
    "[ {\"_id\":1,\"num_users\":56} ]\n",
    "# “_id” represents postcount\n",
    "\n",
    "#postal codes, yes = true, naptan values, street types contain no abbreviations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### pymongo defaults\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1115c65fe9d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def make_pipeline():\n",
    "    # complete the aggregation pipeline\n",
    "    pipeline = [\n",
    "        db.leeds.find().count()\n",
    "                ]\n",
    "    return pipeline\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    db = get_db('maps')\n",
    "    pipeline = make_pipeline()\n",
    "    result = (db, pipeline)\n",
    "    pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576209\n"
     ]
    }
   ],
   "source": [
    "# Number of documents\n",
    "                                                \n",
    "db.char.find().count()                                                \n",
    "\n",
    "\n",
    "import pprint\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def find_simple():\n",
    "    return db.leeds.find().count()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_db('maps')\n",
    "    results = find_simple()\n",
    "    pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.leeds.find().count()\n",
    "576209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488980\n"
     ]
    }
   ],
   "source": [
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def find_simple():\n",
    "    return db.leeds.find({'type':'node'}).count()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_db('maps')\n",
    "    results = find_simple()\n",
    "    pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "db.leeds.find({'type':'node'}).count()\n",
    "488980\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87188\n"
     ]
    }
   ],
   "source": [
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def find_simple():\n",
    "    return db.leeds.find({'type':'way'}).count()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_db('maps')\n",
    "    results = find_simple()\n",
    "    pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87188"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.leeds.find({'type':'way'}).count()\n",
    "87188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'Arthtoach', u'count': 239755}\n"
     ]
    }
   ],
   "source": [
    "## largest contributer\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def find():\n",
    "    return db.leeds.aggregate([{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}},\n",
    "                              {\"$sort\":{\"count\":-1}}, {\"$limit\":1}]) \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_db('maps')\n",
    "    results = find()\n",
    "    pprint.pprint(results.next())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## distinct users\n",
    "\n",
    "(from command line)\n",
    "\n",
    "db.leeds.distinct('created.user').length\n",
    "\n",
    "    669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'count', u'count': 140}\n"
     ]
    }
   ],
   "source": [
    "## users with over 100 contributions\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def find():\n",
    "    return db.leeds.aggregate([{\"$group\":{\"_id\":\"$created.user\", \n",
    "                                          \"count\":{\"$sum\":1}}},\n",
    "                               {\"$match\":{\"count\":{\"$gte\":100}}},\n",
    "                               {\"$group\":{\"_id\":\"count\",\n",
    "                                          \"count\":{\"$sum\":1}}}\n",
    "                               \n",
    "                              \n",
    "                              \n",
    "                              ])\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    get_db('maps')\n",
    "    results = find()\n",
    "    count =0\n",
    "    for result_line in results:\n",
    "        pprint.pprint(result_line)\n",
    "#        count +=1\n",
    "#    print count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'NaPTAN', u'count': 2087}\n",
      "{u'_id': u'JonS', u'count': 564}\n",
      "{u'_id': u'Pobice', u'count': 207}\n",
      "{u'_id': u'LeedsTracker', u'count': 65}\n",
      "{u'_id': u'sc71', u'count': 63}\n"
     ]
    }
   ],
   "source": [
    "### who is submitting naptan data?\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "## top 5 naptan submitters\n",
    "pipeline1 = db.leeds.aggregate([{\"$match\":{\"naptan:\":{\"$exists\":True}}},\n",
    "                                {\"$group\": {\"_id\":\"$created.user\",\n",
    "                                            \"count\":{\"$sum\":1}}},\n",
    "                                {\"$sort\":{\"count\":-1}},\n",
    "                                {\"$limit\":5}\n",
    "                               ])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "{u'_id': u'NaPTAN', u'count': 2087}\n",
    "{u'_id': u'JonS', u'count': 564}\n",
    "{u'_id': u'Pobice', u'count': 207}\n",
    "{u'_id': u'LeedsTracker', u'count': 65}\n",
    "{u'_id': u'sc71', u'count': 63}\n",
    "\"\"\"\n",
    "pipeline2 = db.leeds.aggregate([{\"$match\":{\"naptan:\":{\"$exists\":True}}},\n",
    "                                {\"$group\": {\"_id\":\"$created.user\",\n",
    "                                            \"count\":{\"$sum\":1}}},\n",
    "                                {\"$group\":{\"_id\":\"$null\",\n",
    "                                          \"count\":{\"$sum\":1}}}\n",
    "                               ])\n",
    "#{u'_id': None, u'count': 65}\n",
    "\n",
    "\n",
    "#number of records with naptan\n",
    "pipeline3 = db.leeds.aggregate([{\"$match\":{\"naptan:\":{\"$exists\":True}}},\n",
    "\n",
    "                                {\"$group\":{\"_id\":\"$null\",\n",
    "                                          \"count\":{\"$sum\":1}}}\n",
    "                               ])\n",
    "\n",
    "#{u'_id': None, u'count': 3255}\n",
    "\n",
    "\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    get_db('maps')\n",
    "    results = pipeline1\n",
    "    count =0\n",
    "    for result_line in results:\n",
    "        pprint.pprint(result_line)\n",
    "        \n",
    "    #print results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## largest data points\n",
    "## is there any similarities in who is submitting the largest pieces of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': None, u'count': 9}\n"
     ]
    }
   ],
   "source": [
    "## number of amenities\n",
    "\n",
    "def get_db(db_name):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient('localhost:27017')\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "## number of amenities\n",
    "pipeline1 = db.leeds.aggregate([{\"$match\":{\"amenity\":{\"$exists\":True}}}\n",
    "                               \n",
    "                               ])\n",
    "                               \n",
    "#{u'_id': None, u'count': 5043}\n",
    "\n",
    "\n",
    "## top amenities\n",
    "pipeline2 = db.leeds.aggregate([{\"$match\":{\"amenity\":{\"$exists\":True}}},\n",
    "                               {\"$group\":{\"_id\":\"$amenity\",\n",
    "                                          \"count\":{\"$sum\": 1}}},\n",
    "                                {\"$sort\":{\"count\":-1}},\n",
    "                                {\"$limit\": 20}\n",
    "                               \n",
    "                               ])\n",
    "'''\n",
    "{u'_id': u'parking', u'count': 879}\n",
    "{u'_id': u'post_box', u'count': 696}\n",
    "{u'_id': u'pub', u'count': 360}\n",
    "{u'_id': u'fast_food', u'count': 347}\n",
    "{u'_id': u'place_of_worship', u'count': 242}\n",
    "{u'_id': u'telephone', u'count': 225}\n",
    "{u'_id': u'bench', u'count': 224}\n",
    "{u'_id': u'school', u'count': 221}\n",
    "{u'_id': u'cafe', u'count': 215}\n",
    "{u'_id': u'restaurant', u'count': 198}\n",
    "{u'_id': u'atm', u'count': 145}\n",
    "{u'_id': u'bicycle_parking', u'count': 112}\n",
    "{u'_id': u'waste_basket', u'count': 108}\n",
    "{u'_id': u'pharmacy', u'count': 77}\n",
    "{u'_id': u'bank', u'count': 76}\n",
    "{u'_id': u'post_office', u'count': 74}\n",
    "{u'_id': u'recycling', u'count': 67}\n",
    "{u'_id': u'fuel', u'count': 57}\n",
    "{u'_id': u'bar', u'count': 53}\n",
    "{u'_id': u'doctors', u'count': 50}\n",
    "'''\n",
    "#number of pubs with beer gardens\n",
    "    \n",
    "pipeline3 = db.leeds.aggregate([{\"$match\":{\"beer_garden\":True}},\n",
    "                               {\"$group\":{\"_id\":\"$null\",\n",
    "                                          \"count\":{\"$sum\": 1}}}\n",
    "\n",
    "                               \n",
    "                               ])\n",
    "#{u'_id': None, u'count': 9}\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    get_db('maps')\n",
    "    results = pipeline3\n",
    "\n",
    "    for result_line in results:\n",
    "        pprint.pprint(result_line)\n",
    "        #break\n",
    "\n",
    "## top 5 amenities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#https://upload.wikimedia.org/wikipedia/commons/9/98/LS_postcode_area_map.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "To improve the data overall, it may be useful to have local amenities and business go ahead and make updates, however with the dominance of Google and Facebook in these areas, it may be difficult without further incentives. The best bet for adding further ways and nodes to the map may be with hobbiests deploying generic bots and programs that can pick up data and map it in any given area for example, from photos.\n",
    "\n",
    "Contributers with special with local SME knowledge who are already active on other open source project ie. Wikipedia should also. Government agencies particaularly in transportation could also be encouraged to open source their data via this route. The source for much of the naptan data is \"naptan_import\" which lends a belief that such  an inititative may already be underway at some level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "It seems like the data for the Leeds area is in good shape from a consistency perspective. Street types in the UK are hetergenous to regions, however there were no misspelling. Some of the postcodes are incomplete, only providing the first portion eg. 'LS12', however this still provides some indication of where the node is marked so does not need to be cleaned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
